{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "LordTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRKN1vRYg3Cr"
      },
      "source": [
        "import math\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABeaMc9wbX98"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oZS8xZLbTSj"
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        self.encoder     = TransformerEncoder(\n",
        "            encoder_layer=TransformerEncoderLayer(ninp, nhead, nhid, dropout), \n",
        "            num_layers=nlayers\n",
        "        )\n",
        "        self.embs = nn.Embedding(ntoken, ninp)\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.embs(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.encoder(x, src_mask)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLGVIsZZg3Ct"
      },
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7k-eRYDhmr6"
      },
      "source": [
        "class Reader:\n",
        "    def __init__(self, prefix_path=''):\n",
        "        self.prefix_path = prefix_path\n",
        "\n",
        "    def read_txt(self, path):\n",
        "        path = '{}/{}'.format(self.prefix_path, path)\n",
        "        with open(path, 'r') as f:\n",
        "            return f.readlines()\n",
        "\n",
        "reader   = Reader('/content/drive/MyDrive/colab/advanced_ml')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRitZA46hdC-"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, text):\n",
        "        symbols  = set(text)\n",
        "        self.c2i = { c : i for i, c in enumerate(symbols) }\n",
        "        self.i2c = { i : c for i, c in enumerate(symbols) }\n",
        "\n",
        "    def __getitem__(self, char):\n",
        "        \"\"\"\n",
        "        returns index for given char\n",
        "        \"\"\"\n",
        "        return self.c2i[char]\n",
        "\n",
        "    def __char_by_index__(self, index):\n",
        "        return self.i2c[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        returns length of vocabulary\n",
        "        \"\"\"\n",
        "        return len(self.i2c)\n",
        "\n",
        "    __call__ = __char_by_index__\n",
        "\n",
        "\n",
        "class TextDataset:\n",
        "    def __init__(self, sentences, seqlen=256, lang='RU'):\n",
        "        self._replacements = {\n",
        "            'NL' : lambda x: re.compile('\\n').sub('', x),\n",
        "            '!'  : lambda x: re.compile('!').sub('.', x),\n",
        "            ','  : lambda x: re.compile(',').sub('.', x)\n",
        "        }\n",
        "        if lang == 'RU':\n",
        "            self.replace_bad_chars = lambda x: re.compile(r'[^а-я,\\.\\s]+').sub('', x)\n",
        "        elif lang == 'EN':\n",
        "            self.replace_bad_chars = lambda x: re.compile(r'[^a-z,\\.\\s]+').sub('', x)\n",
        "        else:\n",
        "            print('Unknown text language')\n",
        "\n",
        "        self.text   = self._preprocess(sentences)\n",
        "        self.vocab  = Vocab(self.text)\n",
        "        self.seqlen = seqlen\n",
        "\n",
        "        self.batches = None\n",
        "\n",
        "    def replace(self, sentence):\n",
        "        for replacement in self._replacements.values():\n",
        "            sentence = replacement(sentence)\n",
        "\n",
        "        return sentence\n",
        "    \n",
        "    def _preprocess(self, sentences):\n",
        "        \"\"\"\n",
        "        Preprocesses text\n",
        "        \"\"\"\n",
        "        text = map(self.replace, sentences)\n",
        "        text = map(str.lower, text)\n",
        "        text = map(self.replace_bad_chars, text)\n",
        "        text = map(str.strip, text)\n",
        "        text = filter(lambda t: t, text)\n",
        "        return ' '.join(text)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        \"\"\"\n",
        "        Returns length of associated vocab\n",
        "        \"\"\"\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def encode(self, text=None):\n",
        "        \"\"\"\n",
        "        Runs precomputation chars encodings and paddings\n",
        "        \"\"\"\n",
        "        encoded_text = torch.tensor(\n",
        "            [self.vocab[char] for char in self.text][:self.num_batches * self.seqlen]\n",
        "        )\n",
        "\n",
        "        self.batches = encoded_text.view(self.seqlen, -1).t().contiguous()\n",
        "        self.targets = self.batches[1:]\n",
        "        self.batches = self.batches[:-1]\n",
        "        \n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def num_batches(self):\n",
        "        return len(self.text) // self.seqlen\n",
        "\n",
        "    def _encode(self, sentences):\n",
        "        \"\"\"\n",
        "        Encodes char with with vocab indices.\n",
        "        \"\"\"\n",
        "        return [[self.vocab[char] for char in s] for s in sentences]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HLFbAzzg3Cu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444c5913-22e4-49aa-c8a3-5dbacb11c34a"
      },
      "source": [
        "raw_text = reader.read_txt('lord.txt')\n",
        "\n",
        "data = TextDataset(raw_text, seqlen=32, lang='RU').encode()\n",
        "data.vocab_size, data.num_batches, data.text[:40]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 27438, 'эта сказка возникла в устных рассказах. ')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Fgyn5oivIo"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "val_size=0.05\n",
        "n = int(data.num_batches * val_size)\n",
        "\n",
        "train_data = data.batches[:-2*n].to(device)\n",
        "val_data   = data.batches[-2*n:-n].to(device)\n",
        "test_data  = data.batches[-n:].to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbhWtc-zg3Cw"
      },
      "source": [
        "max_seqlen = 32\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(max_seqlen, len(source) - 1 - i)\n",
        "    data    = source[i:i+seq_len]\n",
        "    target  = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMvweyqig3Cx"
      },
      "source": [
        "ntokens = data.vocab_size \n",
        "emsize  = 512 \n",
        "nhid    = 256 \n",
        "nlayers = 5 \n",
        "nhead   = 8 \n",
        "dropout = 0.3\n",
        "model   = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).cuda()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5slazmbg3Cy"
      },
      "source": [
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "def train():\n",
        "    model.train() \n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    src_mask = model.generate_square_subsequent_mask(max_seqlen).to(device)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, max_seqlen)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        if data.size(0) != max_seqlen:\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 100\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:5.5f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // max_seqlen, scheduler.get_last_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval()\n",
        "    total_loss = 0.\n",
        "    src_mask = model.generate_square_subsequent_mask(max_seqlen).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, max_seqlen):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if data.size(0) != max_seqlen:\n",
        "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
        "            output = eval_model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SnFIqo4g3Cz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688415fc-3d90-42cc-b309-68ddf00ce2a2"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 5 \n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/  771 batches | lr 0.00010 | ms/batch 23.69 | loss  2.67 | ppl    14.38\n",
            "| epoch   1 |   200/  771 batches | lr 0.00010 | ms/batch 22.39 | loss  2.40 | ppl    11.00\n",
            "| epoch   1 |   300/  771 batches | lr 0.00010 | ms/batch 22.46 | loss  2.23 | ppl     9.35\n",
            "| epoch   1 |   400/  771 batches | lr 0.00010 | ms/batch 22.48 | loss  2.13 | ppl     8.41\n",
            "| epoch   1 |   500/  771 batches | lr 0.00010 | ms/batch 22.63 | loss  2.04 | ppl     7.69\n",
            "| epoch   1 |   600/  771 batches | lr 0.00010 | ms/batch 22.69 | loss  1.97 | ppl     7.14\n",
            "| epoch   1 |   700/  771 batches | lr 0.00010 | ms/batch 22.61 | loss  1.92 | ppl     6.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 17.76s | valid loss  1.85 | valid ppl     6.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/  771 batches | lr 0.00009 | ms/batch 22.86 | loss  1.85 | ppl     6.36\n",
            "| epoch   2 |   200/  771 batches | lr 0.00009 | ms/batch 22.63 | loss  1.81 | ppl     6.11\n",
            "| epoch   2 |   300/  771 batches | lr 0.00009 | ms/batch 22.53 | loss  1.77 | ppl     5.87\n",
            "| epoch   2 |   400/  771 batches | lr 0.00009 | ms/batch 22.40 | loss  1.75 | ppl     5.77\n",
            "| epoch   2 |   500/  771 batches | lr 0.00009 | ms/batch 22.45 | loss  1.72 | ppl     5.57\n",
            "| epoch   2 |   600/  771 batches | lr 0.00009 | ms/batch 22.43 | loss  1.69 | ppl     5.44\n",
            "| epoch   2 |   700/  771 batches | lr 0.00009 | ms/batch 22.36 | loss  1.68 | ppl     5.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 17.61s | valid loss  1.67 | valid ppl     5.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  771 batches | lr 0.00008 | ms/batch 22.58 | loss  1.66 | ppl     5.27\n",
            "| epoch   3 |   200/  771 batches | lr 0.00008 | ms/batch 22.36 | loss  1.64 | ppl     5.18\n",
            "| epoch   3 |   300/  771 batches | lr 0.00008 | ms/batch 22.40 | loss  1.62 | ppl     5.06\n",
            "| epoch   3 |   400/  771 batches | lr 0.00008 | ms/batch 22.33 | loss  1.62 | ppl     5.04\n",
            "| epoch   3 |   500/  771 batches | lr 0.00008 | ms/batch 22.32 | loss  1.59 | ppl     4.92\n",
            "| epoch   3 |   600/  771 batches | lr 0.00008 | ms/batch 22.26 | loss  1.58 | ppl     4.87\n",
            "| epoch   3 |   700/  771 batches | lr 0.00008 | ms/batch 22.58 | loss  1.58 | ppl     4.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 17.52s | valid loss  1.60 | valid ppl     4.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  771 batches | lr 0.00007 | ms/batch 22.55 | loss  1.57 | ppl     4.80\n",
            "| epoch   4 |   200/  771 batches | lr 0.00007 | ms/batch 22.29 | loss  1.56 | ppl     4.75\n",
            "| epoch   4 |   300/  771 batches | lr 0.00007 | ms/batch 22.24 | loss  1.54 | ppl     4.67\n",
            "| epoch   4 |   400/  771 batches | lr 0.00007 | ms/batch 22.48 | loss  1.54 | ppl     4.66\n",
            "| epoch   4 |   500/  771 batches | lr 0.00007 | ms/batch 22.31 | loss  1.52 | ppl     4.57\n",
            "| epoch   4 |   600/  771 batches | lr 0.00007 | ms/batch 22.32 | loss  1.51 | ppl     4.54\n",
            "| epoch   4 |   700/  771 batches | lr 0.00007 | ms/batch 22.28 | loss  1.51 | ppl     4.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 17.50s | valid loss  1.56 | valid ppl     4.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  771 batches | lr 0.00007 | ms/batch 22.66 | loss  1.51 | ppl     4.52\n",
            "| epoch   5 |   200/  771 batches | lr 0.00007 | ms/batch 22.35 | loss  1.50 | ppl     4.48\n",
            "| epoch   5 |   300/  771 batches | lr 0.00007 | ms/batch 22.40 | loss  1.48 | ppl     4.41\n",
            "| epoch   5 |   400/  771 batches | lr 0.00007 | ms/batch 22.61 | loss  1.48 | ppl     4.41\n",
            "| epoch   5 |   500/  771 batches | lr 0.00007 | ms/batch 22.46 | loss  1.47 | ppl     4.34\n",
            "| epoch   5 |   600/  771 batches | lr 0.00007 | ms/batch 22.52 | loss  1.46 | ppl     4.32\n",
            "| epoch   5 |   700/  771 batches | lr 0.00007 | ms/batch 22.38 | loss  1.46 | ppl     4.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 17.59s | valid loss  1.53 | valid ppl     4.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/  771 batches | lr 0.00006 | ms/batch 22.68 | loss  1.46 | ppl     4.32\n",
            "| epoch   6 |   200/  771 batches | lr 0.00006 | ms/batch 22.66 | loss  1.46 | ppl     4.29\n",
            "| epoch   6 |   300/  771 batches | lr 0.00006 | ms/batch 22.35 | loss  1.44 | ppl     4.23\n",
            "| epoch   6 |   400/  771 batches | lr 0.00006 | ms/batch 22.49 | loss  1.44 | ppl     4.23\n",
            "| epoch   6 |   500/  771 batches | lr 0.00006 | ms/batch 22.42 | loss  1.43 | ppl     4.16\n",
            "| epoch   6 |   600/  771 batches | lr 0.00006 | ms/batch 22.43 | loss  1.42 | ppl     4.16\n",
            "| epoch   6 |   700/  771 batches | lr 0.00006 | ms/batch 22.48 | loss  1.42 | ppl     4.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 17.60s | valid loss  1.51 | valid ppl     4.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/  771 batches | lr 0.00005 | ms/batch 22.90 | loss  1.43 | ppl     4.16\n",
            "| epoch   7 |   200/  771 batches | lr 0.00005 | ms/batch 22.60 | loss  1.42 | ppl     4.14\n",
            "| epoch   7 |   300/  771 batches | lr 0.00005 | ms/batch 22.80 | loss  1.41 | ppl     4.09\n",
            "| epoch   7 |   400/  771 batches | lr 0.00005 | ms/batch 22.58 | loss  1.41 | ppl     4.08\n",
            "| epoch   7 |   500/  771 batches | lr 0.00005 | ms/batch 22.55 | loss  1.39 | ppl     4.02\n",
            "| epoch   7 |   600/  771 batches | lr 0.00005 | ms/batch 22.68 | loss  1.39 | ppl     4.02\n",
            "| epoch   7 |   700/  771 batches | lr 0.00005 | ms/batch 22.56 | loss  1.39 | ppl     4.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 17.73s | valid loss  1.50 | valid ppl     4.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/  771 batches | lr 0.00005 | ms/batch 22.79 | loss  1.40 | ppl     4.04\n",
            "| epoch   8 |   200/  771 batches | lr 0.00005 | ms/batch 22.42 | loss  1.39 | ppl     4.01\n",
            "| epoch   8 |   300/  771 batches | lr 0.00005 | ms/batch 22.47 | loss  1.38 | ppl     3.97\n",
            "| epoch   8 |   400/  771 batches | lr 0.00005 | ms/batch 22.51 | loss  1.38 | ppl     3.96\n",
            "| epoch   8 |   500/  771 batches | lr 0.00005 | ms/batch 22.50 | loss  1.36 | ppl     3.90\n",
            "| epoch   8 |   600/  771 batches | lr 0.00005 | ms/batch 22.50 | loss  1.36 | ppl     3.90\n",
            "| epoch   8 |   700/  771 batches | lr 0.00005 | ms/batch 22.38 | loss  1.36 | ppl     3.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 17.60s | valid loss  1.49 | valid ppl     4.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/  771 batches | lr 0.00004 | ms/batch 22.93 | loss  1.37 | ppl     3.93\n",
            "| epoch   9 |   200/  771 batches | lr 0.00004 | ms/batch 22.68 | loss  1.36 | ppl     3.91\n",
            "| epoch   9 |   300/  771 batches | lr 0.00004 | ms/batch 22.75 | loss  1.35 | ppl     3.87\n",
            "| epoch   9 |   400/  771 batches | lr 0.00004 | ms/batch 22.51 | loss  1.35 | ppl     3.86\n",
            "| epoch   9 |   500/  771 batches | lr 0.00004 | ms/batch 22.66 | loss  1.34 | ppl     3.80\n",
            "| epoch   9 |   600/  771 batches | lr 0.00004 | ms/batch 22.61 | loss  1.34 | ppl     3.80\n",
            "| epoch   9 |   700/  771 batches | lr 0.00004 | ms/batch 22.46 | loss  1.33 | ppl     3.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 17.71s | valid loss  1.49 | valid ppl     4.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/  771 batches | lr 0.00004 | ms/batch 22.75 | loss  1.34 | ppl     3.84\n",
            "| epoch  10 |   200/  771 batches | lr 0.00004 | ms/batch 22.52 | loss  1.34 | ppl     3.82\n",
            "| epoch  10 |   300/  771 batches | lr 0.00004 | ms/batch 22.38 | loss  1.33 | ppl     3.78\n",
            "| epoch  10 |   400/  771 batches | lr 0.00004 | ms/batch 22.52 | loss  1.33 | ppl     3.77\n",
            "| epoch  10 |   500/  771 batches | lr 0.00004 | ms/batch 22.51 | loss  1.31 | ppl     3.71\n",
            "| epoch  10 |   600/  771 batches | lr 0.00004 | ms/batch 22.59 | loss  1.31 | ppl     3.72\n",
            "| epoch  10 |   700/  771 batches | lr 0.00004 | ms/batch 22.41 | loss  1.31 | ppl     3.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 17.62s | valid loss  1.49 | valid ppl     4.43\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHCdMwOotC6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "d4cbf07b-8d3c-4441-fca3-ee471811b4ed"
      },
      "source": [
        "def gen_char(text, temperature=0):\n",
        "    d = torch.tensor([data.vocab[c] for c in text]).unsqueeze(1).to(device)\n",
        "    m = best_model.generate_square_subsequent_mask(len(text)).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = best_model(d, m)[-1][0]\n",
        "        if temperature != 0:\n",
        "            outputs = nn.Softmax(0)(outputs / temperature)\n",
        "            idx = torch.multinomial(outputs, 1).item()\n",
        "        else:\n",
        "            idx = outputs.argmax().item()\n",
        "        return data.vocab(idx)\n",
        "\n",
        "def generate(text, n, temperature=0):\n",
        "    for _ in range(n):\n",
        "        text += gen_char(text[-data.seqlen:], temperature)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "generate('фродо взял ко', 500, 0.3)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'фродо взял кольцо. на несколько просто он настал и не понял. что он поднял на нем. он долго и начал подножья на получили немного догачет сказать. он повернулся и воды и не смеяться.  сказал он.  он померел подобно полезным и бегу откуда мост верхом. стоящиеся с ним. не было отвердой. но он не подошел к кольца. он воде время он должен был на заполнят должно при поселку. сердце подобное совета. он собирается и постели по сторону вернулся на стоянул свою доль. но он поднял на стоянул свою состуда. он подумал к '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "iq5yah99mAOI",
        "outputId": "1b42e570-1154-4add-c411-b2b85598a019"
      },
      "source": [
        "generate('гендальф был', 400, 0.3)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'гендальф был свет сверкали о нем. он собирается в самом сердце на следуюдым и посмотрел на него западному подножья на стояну и откуда на самом деле на северному склону вернулся в сокружила по нему. он подошел к место. что он сказал он.  наконец он поселкать. он был подножья на солнце. как будто в воду в полной ветер до самого подутия. в долине собирается в сердце и остановились и вершины с ним и посмотрел на '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cnMeQrNWKXT"
      },
      "source": [
        "torch.save(best_model, '/content/drive/MyDrive/colab/advanced_ml/lord.pt')"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}