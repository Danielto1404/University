{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Breakout_new_env.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7s4HJgcY0C90",
    "outputId": "92d29274-7acb-4e02-9a79-a055f050213c"
   },
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install wandb"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: setuptools in /usr/local/lib/python3.7/dist-packages (56.1.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.29)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.1.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GkAPl4claNE"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hxpf9iSecMYf"
   },
   "source": [
    "import torch\n",
    "import math\n",
    "import glob\n",
    "import shutil\n",
    "import io\n",
    "import base64\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import cv2\n",
    "\n",
    "from tqdm import trange\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SB_kKSvza7L"
   },
   "source": [
    "#### Utils for showing atari games"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-eyZLSfdzpgn"
   },
   "source": [
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                    </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(enviroment):\n",
    "    enviroment = Monitor(enviroment, './video', force=True)\n",
    "    return enviroment"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f9XxJu4X05fJ"
   },
   "source": [
    "def play_breakout_and_show_video(model, enviroment):\n",
    "    enviroment = wrap_env(enviroment)\n",
    "    state = enviroment.reset()\n",
    "    total = 0\n",
    "    while True:\n",
    "        enviroment.render()\n",
    "        action = model.choose_action(state)\n",
    "        next_state, reward, done, _ = enviroment.step(action)\n",
    "        state = next_state\n",
    "        total += reward\n",
    "        if done:\n",
    "            enviroment.close()\n",
    "            show_video()\n",
    "            return total"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "UqxzJ6P83T5R",
    "outputId": "d229c993-1527-4456-c0e6-1dc4ea219708"
   },
   "source": [
    "import wandb\n",
    "\n",
    "run_name = 'Dead lives=5, [Huber loss], buffer=80k, lr=1e-4'\n",
    "\n",
    "wandb.init(project='Atari RL', entity='danielto1404', name=run_name)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdanielto1404\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.29<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">Dead lives=5, [Huber loss], buffer=80k, lr=1e-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/danielto1404/Atari%20RL\" target=\"_blank\">https://wandb.ai/danielto1404/Atari%20RL</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/danielto1404/Atari%20RL/runs/35h9x5yn\" target=\"_blank\">https://wandb.ai/danielto1404/Atari%20RL/runs/35h9x5yn</a><br/>\n",
       "                Run data is saved locally in <code>/content/wandb/run-20210505_092625-35h9x5yn</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1f7b972850>"
      ],
      "text/html": [
       "<h1>Run(35h9x5yn)</h1><iframe src=\"https://wandb.ai/danielto1404/Atari%20RL/runs/35h9x5yn\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t46hJC-ylg-x"
   },
   "source": [
    "#### Selector interface for choosing actions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mb9mOPTL1eO-"
   },
   "source": [
    "class ActionSelector:\n",
    "    def __init__(self, model, atari_mode=False, device=None):\n",
    "        super(ActionSelector, self).__init__()\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.atari_mode = atari_mode\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        :return: (best action, Q-value for best action)\n",
    "        \"\"\"\n",
    "        tensor_state = torch.tensor(state).to(self.device)\n",
    "\n",
    "        if self.atari_mode:\n",
    "            tensor_state = tensor_state.unsqueeze(0)\n",
    "\n",
    "        q_values = self.model(tensor_state)\n",
    "        action = torch.argmax(q_values).item()\n",
    "\n",
    "        return action\n",
    "\n",
    "def load_selector(path, atari_mode=True) -> ActionSelector:\n",
    "    return ActionSelector(model=torch.load(path, map_location=torch.device('cpu')),\n",
    "                          atari_mode=atari_mode)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__4-XG3alnrT"
   },
   "source": [
    "#### Epsilon-gready strategy class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9AgpQdYscPyx"
   },
   "source": [
    "class EpsilonStrategy:\n",
    "    def __init__(self, start=1, decay=.999985, min_eps=0.02):\n",
    "        self.eps = start\n",
    "        self.decay = decay\n",
    "        self.start = start\n",
    "        self.min_eps = min_eps\n",
    "\n",
    "    def eps(self):\n",
    "        return self.eps\n",
    "\n",
    "    def decrease(self):\n",
    "        self.eps = max(self.eps * self.decay, self.min_eps)\n",
    "\n",
    "    def check_random_prob(self):\n",
    "        return np.random.random() < self.eps"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ms3P2ZGLlq4D"
   },
   "source": [
    "#### Experience buffer class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "90VUUhoIcTdG"
   },
   "source": [
    "Transition = namedtuple(typename='Transition',\n",
    "                        field_names=['state', 'next_state', 'action', 'reward', 'terminal'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity=10_000, batch_size=32, start_sample_from=10_000):\n",
    "        capacity = int(capacity)\n",
    "        batch_size = int(batch_size)\n",
    "        start_sample_from = int(start_sample_from)\n",
    "\n",
    "        if batch_size > capacity:\n",
    "            raise AssertionError('random sample size should be <= size')\n",
    "\n",
    "        if batch_size > start_sample_from:\n",
    "            raise AssertionError('start sample from should be >= batch_size')\n",
    "\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_indices = np.arange(self.batch_size)\n",
    "        self.start_sample_from = start_sample_from\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        if not self.is_ready_for_sample():\n",
    "            raise AssertionError('Buffer have is not ready for sample')\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), self.batch_size, replace=False)\n",
    "        states, next_states, actions, rewards, terminals = zip(*[self.buffer[i] for i in indices])\n",
    "        return (np.array(states),\n",
    "                np.array(next_states),\n",
    "                np.array(actions, dtype=np.int64),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.array(terminals, dtype=np.uint8))\n",
    "\n",
    "    def store_transition(self, transition: Transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def is_ready_for_sample(self):\n",
    "        return len(self.buffer) >= self.start_sample_from"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdnU_GkrltyV"
   },
   "source": [
    "#### Enviroment wrappers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UeT0ZGa6cZAH"
   },
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, terminal_on_life_loss=False, lives=5):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        self.terminal_on_life_loss = terminal_on_life_loss\n",
    "        self.FIRE_ACTION = 1\n",
    "        self.lives = lives\n",
    "        self.initial_lives = lives\n",
    "        assert env.unwrapped.get_action_meanings()[self.FIRE_ACTION] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        lives = info.get('ale.lives', self.initial_lives)\n",
    "        if lives != self.lives:\n",
    "            self.lives = lives\n",
    "            self.env.step(self.FIRE_ACTION)\n",
    "\n",
    "        return state, reward, done or (lives != self.initial_lives and self.terminal_on_life_loss), info\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(self.FIRE_ACTION)\n",
    "        if done:\n",
    "            self.reset()\n",
    "\n",
    "        way = np.random.choice([2, 3])\n",
    "        for _ in range(np.random.randint(6)):\n",
    "            obs, _, done, _ = self.step(way)\n",
    "        if done:\n",
    "            self.reset()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4, use_for_pool=2):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=use_for_pool)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done, info = None, None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0,\n",
    "                                                high=255,\n",
    "                                                shape=(84, 84, 1),\n",
    "                                                dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, frames, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        self.frames = frames\n",
    "        self.buffer = None\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(frames, axis=0),\n",
    "                                                old_space.high.repeat(frames, axis=0),\n",
    "                                                dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        height, width, frames = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0,\n",
    "                                                high=1.0,\n",
    "                                                shape=(frames, height, width),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation, dtype=np.float32) / 255.0\n",
    "\n",
    "\n",
    "def make_env(env_name, terminal_on_life_loss=True):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env, skip=4, use_for_pool=2)\n",
    "    env = FireResetEnv(env, terminal_on_life_loss=terminal_on_life_loss)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, frames=4)\n",
    "    env = ScaledFloatFrame(env)\n",
    "    return env"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EPhpLDCreh-"
   },
   "source": [
    "#### Action wrappers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DTg5DZHU0Qgj"
   },
   "source": [
    "class ActionWrapper:\n",
    "    def action(self, a):\n",
    "        return a\n",
    "\n",
    "class BreakoutFireDropActionWrapper(ActionWrapper):\n",
    "    def action(self, a):\n",
    "        return 0 if a == 0 else a + 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGAFnGJelwYz"
   },
   "source": [
    "#### DQN Agent class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U5IiARbJcini"
   },
   "source": [
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 env,\n",
    "                 lr=0.0003,\n",
    "                 gamma=0.99,\n",
    "                 loss_function=nn.MSELoss(),\n",
    "                 update_model_frequency=1000,\n",
    "                 experience_buffer=ExperienceBuffer(),\n",
    "                 eps_strategy=EpsilonStrategy(decay=5e-2),\n",
    "                 device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.env = env\n",
    "\n",
    "        # Buffer\n",
    "        self.experience_buffer = experience_buffer\n",
    "\n",
    "        # Constants\n",
    "        self.eps_strategy = eps_strategy\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.update_model_frequency = update_model_frequency\n",
    "        self.lr = lr\n",
    "\n",
    "        # Network\n",
    "        self.loss_function = loss_function\n",
    "        self.device = device\n",
    "\n",
    "        # Default values\n",
    "        self.optimizer = None\n",
    "        self.policy_model, self.target_model = None, None\n",
    "        self.policy_action_selector = None\n",
    "\n",
    "        self.set_policy_model(self.__get_model__())\n",
    "        self.set_target_model(self.__get_model__())\n",
    "\n",
    "    def choose_eps_action(self, state):\n",
    "        \"\"\"\n",
    "        Represents eps-greedy selection according to given eps-strategy.\n",
    "        \"\"\"\n",
    "        if self.eps_strategy.check_random_prob():\n",
    "            return np.random.randint(0, self.action_dim)\n",
    "        else:\n",
    "            return self.choose_action(state)\n",
    "\n",
    "    def choose_action(self, state, model=None):\n",
    "        \"\"\"\n",
    "        :return: best action\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            model = self.policy_model\n",
    "\n",
    "        numpy_state  = np.array([state], copy=False)\n",
    "        tensor_state = torch.tensor(numpy_state).to(self.device)\n",
    "        q_values     = model(tensor_state)\n",
    "        action       = torch.argmax(q_values).item()\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, next_state, action, reward, terminal):\n",
    "        \"\"\"\n",
    "        Stores environment transition in buffer.\n",
    "        \"\"\"\n",
    "        transition = Transition(state=state,\n",
    "                                next_state=next_state,\n",
    "                                action=action,\n",
    "                                reward=reward,\n",
    "                                terminal=terminal)\n",
    "        self.experience_buffer.store_transition(transition)\n",
    "\n",
    "    def learn(self, episode, test_games=(0, 0)):\n",
    "        \"\"\"\n",
    "        Trains Q-function net.\n",
    "        Using fixed target-model to predict target Q-function and q-model to choose actions while training.\n",
    "\n",
    "        test_games -- see doc for 'Trainer'\n",
    "        \"\"\"\n",
    "        test_games_freq, n_test_games = test_games\n",
    "\n",
    "        if test_games_freq != 0 and episode % test_games_freq == 0:\n",
    "            mean_reward = self.play_games(n_test_games)\n",
    "            wandb.log({'test games mean reward' : mean_reward})\n",
    "\n",
    "\n",
    "        if not self.experience_buffer.is_ready_for_sample():\n",
    "            return\n",
    "\n",
    "        states, next_states, actions, rewards, terminals = self.__sample_batch__()\n",
    "\n",
    "        q_eval = self.policy_model(states)[self.experience_buffer.batch_indices, actions]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_future = self.target_model(next_states).max(dim=1).values\n",
    "            q_future[terminals] = 0.0\n",
    "            q_target = rewards + self.gamma * q_future\n",
    "\n",
    "        self.__fit_network__(q_eval, q_target)\n",
    "\n",
    "        if episode % self.update_model_frequency == 0:\n",
    "            self.target_model.load_state_dict(self.policy_model.state_dict())\n",
    "\n",
    "    def play_games(self, n_games):\n",
    "        selector = ActionSelector(self.policy_model, \n",
    "                                  atari_mode=True, \n",
    "                                  device=self.device)\n",
    "        rs = np.zeros(n_games)\n",
    "        for i in range(n_games):\n",
    "            e = make_env('BreakoutNoFrameskip-v4')\n",
    "            s = e.reset()\n",
    "            t = 0\n",
    "            while True:\n",
    "                a = selector.choose_action(s)\n",
    "                next_s, r, done, _ = e.step(a)\n",
    "                s = next_s\n",
    "                t += r\n",
    "                if done:\n",
    "                    rs[i] = t\n",
    "                    break\n",
    "\n",
    "        return rs.mean()\n",
    "\n",
    "    def set_policy_model(self, model):\n",
    "        self.policy_model = model.to(self.device)  \n",
    "        self.optimizer = torch.optim.Adam(self.policy_model.parameters(), \n",
    "                                          lr=self.lr)\n",
    "\n",
    "    def set_target_model(self, model):\n",
    "        self.target_model = model.to(self.device)\n",
    "\n",
    "    def __get_model__(self):\n",
    "        pass\n",
    "\n",
    "    def __sample_batch__(self):\n",
    "        \"\"\"\n",
    "        :return: ( [states], [next_states], [actions], [rewards], [is_done] )\n",
    "        \"\"\"\n",
    "        states, next_states, actions, rewards, terminals = self.experience_buffer.sample_batch()\n",
    "        return [\n",
    "            torch.FloatTensor(states).to(self.device),\n",
    "            torch.FloatTensor(next_states).to(self.device),\n",
    "            torch.LongTensor(actions).to(self.device),\n",
    "            torch.FloatTensor(rewards).to(self.device),\n",
    "            torch.BoolTensor(terminals).to(self.device)\n",
    "        ]\n",
    "\n",
    "    def __fit_network__(self, q_eval, q_target):\n",
    "        loss = self.loss_function(q_eval, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_value_(self.policy_model.parameters(), clip_value=1)\n",
    "        self.optimizer.step()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXX544zAlzeN"
   },
   "source": [
    "#### Atari CNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l5MYLFn_rGjn"
   },
   "source": [
    "class AtariCNN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariCNN, self).__init__()\n",
    "\n",
    "        frames, width, height = input_shape\n",
    "\n",
    "        self.conv1  = nn.Conv2d(frames, 32, kernel_size=(8, 8), stride=(4, 4))\n",
    "        self.conv2  = nn.Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
    "        self.conv3  = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "\n",
    "        self.dense1 = nn.Linear(self.__get_dense_shape__(input_shape), 512)\n",
    "        self.dense2 = nn.Linear(512, n_actions)\n",
    "\n",
    "        torch.nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.dense1.weight)\n",
    "        torch.nn.init.kaiming_normal_(self.dense2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.02)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.02)\n",
    "        x = F.leaky_relu(self.conv3(x), 0.02)\n",
    "        x = F.leaky_relu(self.dense1(x.view(x.shape[0], -1)), 0.02)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "    def conv(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "    def __get_dense_shape__(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size())) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVMjEPX9mD9D"
   },
   "source": [
    "#### Atari agent and trainer classes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q_YrbBsbc4mk"
   },
   "source": [
    "class AtariAgent(Agent):\n",
    "    def __get_model__(self):\n",
    "        return AtariCNN(input_shape=self.state_dim, n_actions=self.action_dim)\n",
    "\n",
    "class AtariAgentTrainer:\n",
    "    def __init__(self,\n",
    "                 agent: Agent,\n",
    "                 episodes,\n",
    "                 action_wrapper=ActionWrapper()):\n",
    "        self.agent = agent\n",
    "        self.episodes = episodes\n",
    "        self.action_wrapper = action_wrapper\n",
    "\n",
    "    def train(self, path='trained_models/atari', test_games=(0, 0), max_episodes_per_game=None):\n",
    "        \"\"\"\n",
    "        path                 : path for saving model\n",
    "        test_games           : (test_games_frequency, amount of games)\n",
    "        max_episodes_per_game: amount of episodes for stoping long game.\n",
    "        \"\"\"\n",
    "        rewards, test_rewards = [], []\n",
    "        game_reward, best_mean_reward, game_episode = 0, 0, 0\n",
    "        state = self.agent.env.reset()\n",
    "        progress = trange(self.episodes, desc='epochs')\n",
    "\n",
    "        def check_for_stop_game(game_episode_):\n",
    "            if max_episodes_per_game is None:\n",
    "                return False\n",
    "            return game_episode_ > max_episodes_per_game\n",
    "\n",
    "        for episode in progress:\n",
    "            self.agent.eps_strategy.decrease()\n",
    "            game_episode += 1\n",
    "\n",
    "            action  = self.agent.choose_eps_action(state)\n",
    "            wrapped_action = self.action_wrapper.action(action)\n",
    "            next_state, reward, done, _ = self.agent.env.step(wrapped_action)\n",
    "\n",
    "            game_reward += reward\n",
    "\n",
    "            self.agent.store_transition(state=state,\n",
    "                                        next_state=next_state,\n",
    "                                        action=action,\n",
    "                                        terminal=done,\n",
    "                                        reward=reward)\n",
    "\n",
    "            self.agent.learn(episode=episode, test_games=test_games)\n",
    "\n",
    "            if done or check_for_stop_game(game_episode):\n",
    "                rewards.append(game_reward)\n",
    "                game_reward, game_episode = 0, 0\n",
    "\n",
    "                mean_reward = np.mean(rewards[-100:])\n",
    "\n",
    "                if mean_reward > best_mean_reward:\n",
    "                    best_mean_reward = mean_reward\n",
    "                    torch.save(self.agent.policy_model, '{}_best.pt'.format(path))\n",
    "\n",
    "                wandb.log({\n",
    "                    'mean reward': mean_reward,\n",
    "                    'epsilon' : self.agent.eps_strategy.eps,\n",
    "                    'last game reward': rewards[-1]\n",
    "                })\n",
    "\n",
    "                progress_status = \"last game reward: {} | games: {} | mean reward: {:02f} | epsilon: {:02f}\".format(\n",
    "                    rewards[-1], len(rewards), mean_reward, self.agent.eps_strategy.eps\n",
    "                )\n",
    "        \n",
    "                progress.set_postfix_str(progress_status)\n",
    "\n",
    "                state = self.agent.env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        torch.save(self.agent.policy_model, '{}.pt'.format(path))\n",
    "        return rewards"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sZQ7jifeyuCG"
   },
   "source": [
    "def preload_model(agent, path):\n",
    "    agent.set_policy_model(torch.load(path))\n",
    "    agent.set_target_model(torch.load(path))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8W09uCxuc6_D"
   },
   "source": [
    "env_name = 'BreakoutNoFrameskip-v4'\n",
    "\n",
    "env = make_env(env_name, terminal_on_life_loss=False)\n",
    "\n",
    "atari_agent = AtariAgent(state_dim=(4, 84, 84),\n",
    "                         action_dim=3,\n",
    "                         lr=1e-4,\n",
    "                         env=env,   \n",
    "                         update_model_frequency=1_000,\n",
    "                         loss_function=nn.SmoothL1Loss(),\n",
    "                         experience_buffer=ExperienceBuffer(capacity=80_000,\n",
    "                                                            batch_size=32,\n",
    "                                                            start_sample_from=30_000),\n",
    "                         \n",
    "                         eps_strategy=EpsilonStrategy(start=1, decay=.999990, min_eps=0.1)\n",
    "                         )\n",
    "\n",
    "\n",
    "# preload_model(atari_agent, '/content/atari-xx_best.pt')\n",
    "\n",
    "\n",
    "trainer = AtariAgentTrainer(agent=atari_agent,                         \n",
    "                            episodes=1_500_000,\n",
    "                            action_wrapper=BreakoutFireDropActionWrapper())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XcBMv4cd75Z"
   },
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pz3o9kJthrNS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8c50a51e-8649-4da5-a8f2-f1ee7ae4de73"
   },
   "source": [
    "test_rewards = trainer.train(path='breakout-dead-lives-colab')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "epochs:  91%|█████████ | 1367321/1500000 [4:58:02<29:13, 75.66it/s, last game reward: 53.0 | games: 3659 | mean reward: 30.030000 | epsilon: 0.100000]"
     ],
     "name": "stderr"
    }
   ]
  }
 ]
}